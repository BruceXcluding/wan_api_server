import os
import logging
import torch
import torch.distributed as dist
from PIL import Image
from .base_pipeline import BasePipeline

try:
    import torch_npu
    from torch_npu.contrib import transfer_to_npu
    NPU_AVAILABLE = True
except ImportError:
    NPU_AVAILABLE = False

import wan
from wan.configs import WAN_CONFIGS, MAX_AREA_CONFIGS

logger = logging.getLogger(__name__)

class NPUPipeline(BasePipeline):
    """åä¸ºæ˜‡è…¾ NPU è§†é¢‘ç”Ÿæˆç®¡é“ - æ”¯æŒåˆ†å¸ƒå¼"""

    def __init__(self, ckpt_dir: str, rank=0, world_size=1, use_distributed=False, **model_args):
        if not NPU_AVAILABLE:
            raise RuntimeError("torch_npu not available, cannot use NPU pipeline")
        
        # ğŸ”¥ åœ¨è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°å‰è®¾ç½®device_type
        model_args['device_type'] = 'npu'  # ğŸ”¥ å¼ºåˆ¶è®¾ç½®ä¸ºnpu
        
        # ğŸ”¥ è®¾ç½®åˆ†å¸ƒå¼å‚æ•°
        self.rank = rank
        self.world_size = world_size  
        self.local_rank = int(os.environ.get("LOCAL_RANK", rank))
        self.use_distributed = use_distributed
        
        # åˆ†å¸ƒå¼é…ç½®
        self.t5_fsdp = use_distributed
        self.dit_fsdp = use_distributed
        self.ulysses_size = world_size if use_distributed else 1
        self.vae_parallel = use_distributed
        self.t5_cpu = model_args.get("t5_cpu", False)
        
        # ğŸ”¥ è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼Œä¼ å…¥ä¿®æ­£åçš„model_args
        super().__init__(ckpt_dir, **model_args)

    def _get_backend(self) -> str:
        return "hccl"

    def _load_model(self):
        """åŠ è½½åˆ†å¸ƒå¼æ¨¡å‹"""
        # è®¾ç½®è®¾å¤‡
        torch_npu.npu.set_device(self.local_rank)
        logger.info(f"Rank {self.rank}: Loading WanI2V on NPU:{self.local_rank}")
        
        # åŠ è½½é…ç½®
        cfg = WAN_CONFIGS.get("i2v-14B")
        if not cfg:
            raise ValueError("i2v-14B config not found")
        
        # åˆ›å»ºåˆ†å¸ƒå¼æ¨¡å‹
        model = wan.WanI2V(
            config=cfg,
            checkpoint_dir=self.ckpt_dir,
            device_id=self.local_rank,
            rank=self.rank,
            t5_fsdp=self.t5_fsdp,
            dit_fsdp=self.dit_fsdp,
            use_usp=(self.ulysses_size > 1),
            t5_cpu=self.t5_cpu,
            use_vae_parallel=self.vae_parallel,
        )
        
        logger.info(f"Rank {self.rank}: WanI2V loaded with distributed config: "
                   f"t5_fsdp={self.t5_fsdp}, dit_fsdp={self.dit_fsdp}, "
                   f"ulysses_size={self.ulysses_size}, vae_parallel={self.vae_parallel}")
        
        return model

    def _generate_video_device_specific(self, request, img, progress_callback=None):
        """NPUè®¾å¤‡ç‰¹å®šçš„è§†é¢‘ç”Ÿæˆ"""
        logger.info(f"Rank {self.rank}: Starting distributed video generation")
        
        # è§£æè¯·æ±‚å‚æ•°
        height, width = map(int, getattr(request, "image_size", "1280*720").split("*"))
        max_area = width * height

        # ğŸ”¥ è®°å½•è´Ÿé¢æç¤ºè¯ä½†ä¸ä½¿ç”¨ï¼ˆå› ä¸ºWanI2Vä¸æ”¯æŒï¼‰
        negative_prompt = getattr(request, "negative_prompt", "")
        if negative_prompt and self.rank == 0:
            logger.warning(f"negative_prompt '{negative_prompt}' ignored - WanI2V doesn't support this parameter") 
        
        # åªæœ‰rank 0è¾“å‡ºè¯¦ç»†æ—¥å¿—
        if self.rank == 0:
            logger.info(f"Generating video: {width}x{height}, {getattr(request, 'num_frames', 81)} frames")
            
        # ğŸ”¥ ä¿®å¤ï¼šå‚ç…§generate.pyç¬¬311è¡Œçš„ç²¾ç¡®å‚æ•°æ˜ å°„
        video = self.model.generate(
            request.prompt,                                    # ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ï¼šprompt
            img,                                              # ç¬¬äºŒä¸ªä½ç½®å‚æ•°ï¼šimg
            max_area=max_area,                                # å…³é”®å­—å‚æ•°
            frame_num=getattr(request, "num_frames", 81),
            shift=getattr(request, "sample_shift", 5.0),      # âœ… schemaä¸­æœ‰è¿™ä¸ªå­—æ®µ
            sample_solver=getattr(request, "sample_solver", "unipc"),  # âœ… schemaä¸­æœ‰è¿™ä¸ªå­—æ®µ
            sampling_steps=getattr(request, "infer_steps", 40),  # ğŸ”¥ ä¿®å¤ï¼šinfer_steps -> sampling_steps
            guide_scale=getattr(request, "guidance_scale", 5.0),  # ğŸ”¥ ä¿®å¤ï¼šguidance_scale -> guide_scale
            seed=getattr(request, "seed", 42) if getattr(request, "seed", None) is not None else 42,  # ğŸ”¥ é˜²æ­¢None
            offload_model=False,  # ğŸ”¥ ä¿®å¤ï¼šåˆ†å¸ƒå¼æ—¶å›ºå®šä¸ºFalse
        )
    
        if self.rank == 0:
            logger.info(f"Distributed video generation completed")
            
        return video

    def _save_video(self, video_tensor, output_path: str):
        """ä¿å­˜è§†é¢‘ - åªæœ‰rank 0ä¿å­˜"""
        if self.rank == 0:
            logger.info(f"Rank 0: Saving video to {output_path}")
            try:
                from wan.utils.utils import cache_video
                cache_video(
                    tensor=video_tensor[None] if video_tensor.ndim == 4 else video_tensor,
                    save_file=output_path,
                    fps=video_tensor.shape[0] // 5,
                    nrow=1,
                    normalize=True,
                    value_range=(-1, 1)
                )
                logger.info(f"Video saved successfully: {output_path}")
            except Exception as e:
                logger.error(f"Failed to save video: {e}")
                # åˆ›å»ºå ä½æ–‡ä»¶
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, "wb") as f:
                    f.write(b"FAKE_VIDEO_DATA")
        else:
            # å…¶ä»–rankç­‰å¾…rank 0å®Œæˆä¿å­˜
            if dist.is_initialized():
                dist.barrier()

    def _log_memory_usage(self):
        """è®°å½•NPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            memory_allocated = torch_npu.npu.memory_allocated(self.local_rank) / 1024**3
            memory_reserved = torch_npu.npu.memory_reserved(self.local_rank) / 1024**3
            logger.info(f"Rank {self.rank} NPU:{self.local_rank} memory: "
                       f"{memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved")
        except Exception as e:
            logger.warning(f"Rank {self.rank}: Failed to get NPU memory info: {e}")

    def _empty_cache(self):
        """æ¸…ç©ºNPUç¼“å­˜"""
        torch_npu.npu.empty_cache()
        # åˆ†å¸ƒå¼åŒæ­¥
        if dist.is_initialized():
            dist.barrier()

    def generate_video(self, request, task_id):
        """ç”Ÿæˆè§†é¢‘çš„ä¸»å…¥å£"""
        try:
            # å¤„ç†å›¾ç‰‡è¾“å…¥
            if hasattr(request, 'image_url') and request.image_url:
                if request.image_url.startswith("http"):
                    import requests
                    from io import BytesIO
                    response = requests.get(request.image_url)
                    img = Image.open(BytesIO(response.content))
                else:
                    img = Image.open(request.image_url)
            else:
                raise ValueError("image_url is required")

            # ç”Ÿæˆè§†é¢‘
            video_tensor = self._generate_video_device_specific(request, img)
            
            # ä¿å­˜è§†é¢‘
            output_path = f"generated_videos/{task_id}.mp4"
            os.makedirs("generated_videos", exist_ok=True)
            self._save_video(video_tensor, output_path)
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            self._log_memory_usage()
            
            return f"/videos/{task_id}.mp4"
            
        except Exception as e:
            logger.error(f"Rank {self.rank}: Video generation failed: {e}")
            raise

    def reload_model(self):
        """é‡æ–°åŠ è½½æ¨¡å‹"""
        logger.info(f"Rank {self.rank}: Reloading model...")
        self._empty_cache()
        self.model = self._load_model()
        logger.info(f"Rank {self.rank}: Model reloaded successfully")