import os
import sys
import torch
import torch.distributed as dist
from datetime import datetime, timedelta

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)

import logging
import uuid
import threading
import time
from typing import List
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import uvicorn

from schemas import VideoSubmitRequest, VideoSubmitResponse, VideoStatusResponse, TaskStatus
from pipelines.pipeline_factory import create_pipeline
from utils.device_detector import detect_device
from utils.load_monitor import LoadMonitor
from utils.dynamic_scheduler import DynamicGPUScheduler

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
task_queue = []
status_dict = {}
cancelled_tasks = set()
pipeline = None
load_monitor = None
dynamic_scheduler = None

def init_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    try:
        import torch_npu
        npu_available = torch_npu.npu.is_available()
    except ImportError:
        npu_available = False
    
    # è·å–åˆ†å¸ƒå¼ä¿¡æ¯
    rank = int(os.environ.get("RANK", 0))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    logger.info(f"Initializing rank {rank}/{world_size}, local_rank {local_rank}")
    
    if world_size > 1:
        # ğŸ”¥ å…ˆè®¾ç½®è®¾å¤‡å†åˆå§‹åŒ–åˆ†å¸ƒå¼
        if npu_available:
            import torch_npu
            torch_npu.npu.set_device(local_rank)  # ğŸ”¥ ä½¿ç”¨torch_npu.npu.set_device
            logger.info(f"Rank {rank}: Set NPU device {local_rank}")
        elif torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
            logger.info(f"Rank {rank}: Set CUDA device {local_rank}")
        
        # ğŸ”¥ ä½¿ç”¨æ­£ç¡®çš„åˆ†å¸ƒå¼åˆå§‹åŒ–
        backend = "hccl" if npu_available else "nccl" if torch.cuda.is_available() else "gloo"
        
        try:
            if not dist.is_initialized():
                dist.init_process_group(
                    backend=backend,
                    timeout=timedelta(seconds=300),  # ğŸ”¥ æ·»åŠ è¶…æ—¶
                    init_method='env://'  # ğŸ”¥ æ˜ç¡®æŒ‡å®š
                )
                logger.info(f"Rank {rank}: Distributed initialized with {backend}")
            else:
                logger.info(f"Rank {rank}: Distributed already initialized")
        except Exception as e:
            logger.error(f"Rank {rank}: Failed to initialize distributed: {e}")
            raise
    else:
        logger.info(f"Rank {rank}: Single device mode")
    
    return rank, local_rank, world_size

def create_app():
    """åˆ›å»ºFastAPIåº”ç”¨"""
    app = FastAPI(title="Wan2.1 I2V Multi-Device API Server")
    app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])
    
    os.makedirs("generated_videos", exist_ok=True)
    app.mount("/videos", StaticFiles(directory="generated_videos"), name="videos")

    @app.post("/submit", response_model=VideoSubmitResponse)
    async def submit(request: VideoSubmitRequest):
        task_id = f"req_{uuid.uuid4().hex[:16]}"
        status_dict[task_id] = {
            "status": TaskStatus.PENDING, 
            "result_url": "", 
            "error": "",
            "created_at": datetime.now().isoformat(),
            "progress": 0
        }
        task_queue.append((task_id, request))
        logger.info(f"Task submitted: {task_id}")
        return VideoSubmitResponse(requestId=task_id, status=TaskStatus.PENDING, message="ä»»åŠ¡å·²æäº¤", estimated_time=30)

    @app.post("/batch_submit", response_model=List[VideoSubmitResponse])
    async def batch_submit(requests: List[VideoSubmitRequest]):
        responses = []
        for req in requests:
            task_id = f"req_{uuid.uuid4().hex[:16]}"
            status_dict[task_id] = {
                "status": TaskStatus.PENDING, 
                "result_url": "", 
                "error": "",
                "created_at": datetime.now().isoformat(),
                "progress": 0
            }
            task_queue.append((task_id, req))
            responses.append(VideoSubmitResponse(requestId=task_id, status=TaskStatus.PENDING, message="ä»»åŠ¡å·²æäº¤", estimated_time=30))
        return responses

    # ğŸ”¥ æ–°å¢ï¼šå–æ¶ˆä»»åŠ¡
    @app.post("/cancel/{task_id}")
    async def cancel_task(task_id: str):
        if task_id not in status_dict:
            raise HTTPException(status_code=404, detail="Task not found")
        
        current_status = status_dict[task_id]["status"]
        if current_status in [TaskStatus.SUCCEED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
            raise HTTPException(status_code=400, detail=f"Task already {current_status.value}")
        
        cancelled_tasks.add(task_id)
        status_dict[task_id]["status"] = TaskStatus.CANCELLED
        status_dict[task_id]["updated_at"] = datetime.now().isoformat()
        
        # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
        global task_queue
        task_queue = [(tid, req) for tid, req in task_queue if tid != task_id]
        
        logger.info(f"Task {task_id} cancelled")
        return {"message": f"Task {task_id} cancelled successfully"}

    @app.post("/cancel_all")
    async def cancel_all():
        cancelled_count = 0
        for task_id, task_info in status_dict.items():
            if task_info["status"] in [TaskStatus.PENDING, TaskStatus.RUNNING]:
                cancelled_tasks.add(task_id)
                status_dict[task_id]["status"] = TaskStatus.CANCELLED
                status_dict[task_id]["updated_at"] = datetime.now().isoformat()
                cancelled_count += 1
        
        global task_queue
        task_queue = []
        
        logger.info(f"Cancelled {cancelled_count} tasks")
        return {"message": f"Cancelled {cancelled_count} tasks"}

    @app.get("/status/{task_id}", response_model=VideoStatusResponse)
    async def status(task_id: str):
        s = status_dict.get(task_id)
        if not s:
            raise HTTPException(status_code=404, detail="Task not found")

        # ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€æ¶ˆæ¯
        message = "ä»»åŠ¡å¤„ç†ä¸­"
        if s.get("status") == TaskStatus.RUNNING:
            stage = s.get("current_stage", "å¤„ç†ä¸­")
            progress = s.get("progress", 0)
            message = f"{stage} ({progress:.1f}%)"
        elif s.get("status") == TaskStatus.FAILED:
            message = s.get("error", "ç”Ÿæˆå¤±è´¥")
        elif s.get("status") == TaskStatus.SUCCEED:
            message = "ç”Ÿæˆå®Œæˆ"
        elif s.get("status") == TaskStatus.CANCELLED:
            message = "ä»»åŠ¡å·²å–æ¶ˆ"

        return VideoStatusResponse(
            requestId=task_id,
            status=s.get("status", TaskStatus.PENDING),
            progress=s.get("progress", 0),
            message=message,  # ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨åŠ¨æ€æ¶ˆæ¯
            created_at=s.get("created_at", ""),
            updated_at=s.get("updated_at", ""),
            results=s.get("results"),
            reason=s.get("reason"),
            elapsed_time=s.get("elapsed_time"),
            current_stage=s.get("current_stage"),  # ğŸ”¥ æ–°å¢
        )

    @app.get("/health")
    async def health():
        dtype, dcount, backend = detect_device()
        return {
            "device_type": dtype, 
            "device_count": dcount, 
            "backend": backend, 
            "queue_size": len(task_queue), 
            "total_tasks": len(status_dict),
            "cancelled_tasks": len(cancelled_tasks)
        }
    
    @app.get("/monitor")
    async def monitor():
        """ç®€å•çš„ä»»åŠ¡ç›‘æ§"""
        running = [{"id": k, "progress": v.get("progress", 0), "stage": v.get("current_stage", "")} 
                   for k, v in status_dict.items() if v.get("status") == TaskStatus.RUNNING]

        return {
            "queue_size": len(task_queue),
            "running_tasks": running,
            "total_tasks": len(status_dict)
        }


    @app.get("/load/status")
    async def get_load_status():
        """è·å–è´Ÿè½½çŠ¶æ€"""
        global dynamic_scheduler
        if not dynamic_scheduler:
            return {
                "load_balancing": False,
                "message": "Load balancing not available"
            }
        
        loads = dynamic_scheduler.load_monitor.get_current_loads()
        status = dynamic_scheduler.load_monitor.get_load_status()
        
        return {
            "load_balancing": True,
            "device_count": dynamic_scheduler.world_size,
            "device_type": dynamic_scheduler.device_type,
            "status": status,
            "details": {
                str(rank): {
                    "memory_used_gb": round(load.memory_used_gb, 2),
                    "memory_total_gb": round(load.memory_total_gb, 2),
                    "utilization": round(load.utilization, 3),
                    "load_score": round(load.load_score, 3)
                }
                for rank, load in loads.items()
            },
            "active_tasks": len(status_dict)
        }

    @app.get("/load/metrics")
    async def get_load_metrics():
        """è·å–è¯¦ç»†è´Ÿè½½æŒ‡æ ‡"""
        global dynamic_scheduler
        if not dynamic_scheduler:
            return {"error": "Load monitor not available"}
        
        loads = dynamic_scheduler.load_monitor.get_current_loads()
        return {
            "timestamp": time.time(),
            "device_count": dynamic_scheduler.world_size,
            "device_type": dynamic_scheduler.device_type,
            "devices": {
                str(rank): {
                    "memory_used_gb": round(load.memory_used_gb, 2),
                    "memory_total_gb": round(load.memory_total_gb, 2),
                    "utilization": round(load.utilization, 3),
                    "load_score": round(load.load_score, 3),
                    "last_update": load.last_update
                }
                for rank, load in loads.items()
            }
        }

    @app.get("/scheduler/status")
    async def get_scheduler_status():
        """è·å–åŠ¨æ€è°ƒåº¦å™¨çŠ¶æ€"""
        global dynamic_scheduler
        if dynamic_scheduler:
            return dynamic_scheduler.get_scheduler_status()
        else:
            return {"dynamic_scheduling": False, "message": "Dynamic scheduling not enabled"}

    @app.get("/cluster/health")
    async def get_cluster_health():
        """è·å–é›†ç¾¤å¥åº·çŠ¶æ€"""
        global dynamic_scheduler
        health_info = {
            "status": "healthy",
            "timestamp": time.time(),
            "active_tasks": len(status_dict),
            "queue_size": len(task_queue),
            "load_balancing": dynamic_scheduler is not None
        }
        
        if dynamic_scheduler:
            status = dynamic_scheduler.load_monitor.get_load_status()
            health_info.update({
                "device_status": {
                    "total_devices": dynamic_scheduler.world_size,
                    "available": len(status["available"]),
                    "busy": len(status["busy"]),
                    "overloaded": len(status["overloaded"])
                }
            })
            
            # å¥åº·çŠ¶æ€è¯„ä¼°
            if len(status["overloaded"]) > dynamic_scheduler.world_size * 0.7:
                health_info["status"] = "critical"
            elif len(status["overloaded"]) > dynamic_scheduler.world_size * 0.3:
                health_info["status"] = "warning"
        
        return health_info

    return app

def process_tasks():
    """å¤„ç†ä»»åŠ¡é˜Ÿåˆ— - æ”¯æŒåŠ¨æ€GPUè°ƒåº¦"""
    global dynamic_scheduler
    
    while task_queue:
        task_id, request = task_queue.pop(0)
        
        if task_id in cancelled_tasks:
            logger.info(f"Task {task_id} was cancelled, skipping")
            continue
            
        try:
            logger.info(f"ğŸš€ Processing task with dynamic scheduling: {task_id}")
            
            # ğŸ”¥ åŠ¨æ€GPUè°ƒåº¦
            assigned_ranks = None
            if dynamic_scheduler:
                assigned_ranks = dynamic_scheduler.schedule_task(task_id, request)
                
                if assigned_ranks is None:
                    # ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—ï¼Œç­‰å¾…GPUèµ„æº
                    logger.info(f"Task {task_id}: Queued, waiting for GPU resources")
                    time.sleep(5)  # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                    continue
                
                logger.info(f"Task {task_id}: Assigned to GPUs {assigned_ranks}")
            else:
                # å›é€€åˆ°å…¨GPUæ¨¡å¼
                assigned_ranks = list(range(int(os.environ.get("WORLD_SIZE", 1))))
            
            status_dict[task_id]["status"] = TaskStatus.RUNNING
            status_dict[task_id]["updated_at"] = datetime.now().isoformat()
            status_dict[task_id]["start_time"] = datetime.now()
            status_dict[task_id]["assigned_gpus"] = assigned_ranks  # ğŸ”¥ è®°å½•åˆ†é…çš„GPU
            
            # ğŸ”¥ åªå¹¿æ’­ç»™åˆ†é…çš„ranks
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            if world_size > 1 and len(assigned_ranks) < world_size:
                # åŠ¨æ€å­ç»„æ¨¡å¼
                task_data = {
                    'request': request, 
                    'task_id': task_id,
                    'assigned_ranks': assigned_ranks,
                    'dynamic_mode': True
                }
                
                # ğŸ”¥ å‘é€ç»™æ‰€æœ‰rankï¼Œè®©å®ƒä»¬è‡ªå·±åˆ¤æ–­æ˜¯å¦å‚ä¸
                broadcast_data = [task_data]
                dist.broadcast_object_list(broadcast_data, src=0)
                logger.info(f"Task {task_id}: Dynamic assignment broadcasted")
                
            elif world_size > 1:
                # å…¨GPUæ¨¡å¼
                task_data = [{'request': request, 'task_id': task_id}]
                dist.broadcast_object_list(task_data, src=0)
                logger.info(f"Task {task_id}: Full GPU mode broadcasted")
            
            def progress_callback(progress, stage="Processing"):
                if task_id in cancelled_tasks:
                    raise Exception("Task was cancelled")
                status_dict[task_id]["progress"] = progress
                status_dict[task_id]["current_stage"] = stage
                
                elapsed = datetime.now() - status_dict[task_id]["start_time"]
                logger.info(f"ğŸ“Š Task {task_id}: {stage} ({progress:.1f}%) - GPUs: {assigned_ranks} - Elapsed: {elapsed}")
                return progress
            
            # ğŸ”¥ æ‰§è¡Œä»»åŠ¡
            current_rank = int(os.environ.get("RANK", 0))
            if current_rank in assigned_ranks:
                # åªæœ‰è¢«åˆ†é…çš„rankæ‰§è¡Œä»»åŠ¡
                result = pipeline.generate_video(request, task_id, progress_callback=progress_callback)
            else:
                # å…¶ä»–rankè·³è¿‡è¿™ä¸ªä»»åŠ¡
                result = None
            
            # ğŸ”¥ ä»»åŠ¡å®Œæˆï¼Œé‡Šæ”¾GPUèµ„æº
            if dynamic_scheduler:
                dynamic_scheduler.release_gpus(task_id)
            
            # ğŸ”¥ æ·»åŠ ï¼šä»»åŠ¡å®Œæˆåç­‰å¾…æ‰€æœ‰rankåŒæ­¥
            if world_size > 1:
                import torch.distributed as dist
                if dist.is_initialized():
                    try:
                        dist.barrier(timeout=timedelta(seconds=30))
                        logger.info(f"Task {task_id}: All ranks synchronized after completion")
                    except Exception as e:
                        logger.warning(f"Task {task_id}: Post-completion barrier failed: {e}")
                        
                    # ğŸ”¥ æ–°æ·»åŠ ï¼šå‘é€å®Œæˆä¿¡å·ï¼Œè®©å…¶ä»–rankç»“æŸå½“å‰ä»»åŠ¡ç­‰å¾…
                    try:
                        completion_signal = [{"type": "TASK_COMPLETED", "task_id": task_id}]
                        dist.broadcast_object_list(completion_signal, src=0)
                        logger.info(f"Task {task_id}: Completion signal sent to all ranks")
                    except Exception as e:
                        logger.warning(f"Task {task_id}: Failed to send completion signal: {e}")
             
            # æœ€åæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦è¢«å–æ¶ˆ
            if task_id in cancelled_tasks:
                logger.info(f"Task {task_id} was cancelled during processing")
                continue
            
            # ğŸ”¥ æ–°å¢ï¼šè®¡ç®—æ€»è€—æ—¶
            total_time = datetime.now() - status_dict[task_id]["start_time"]
            
            status_dict[task_id]["status"] = TaskStatus.SUCCEED
            status_dict[task_id]["result_url"] = result
            status_dict[task_id]["updated_at"] = datetime.now().isoformat()
            status_dict[task_id]["elapsed_time"] = str(total_time)  # ğŸ”¥ æ–°å¢
            logger.info(f"âœ… Task {task_id} completed successfully in {total_time}")
            
        except Exception as e:
            # ğŸ”¥ å¼‚å¸¸æ—¶ä¹Ÿè¦é‡Šæ”¾èµ„æº
            if dynamic_scheduler:
                dynamic_scheduler.release_gpus(task_id)
            
            # ğŸ”¥ æ–°å¢ï¼šå¼‚å¸¸æ—¶ä¹Ÿè®°å½•è€—æ—¶å’Œå‘é€å®Œæˆä¿¡å·
            if "start_time" in status_dict[task_id]:
                total_time = datetime.now() - status_dict[task_id]["start_time"]
                status_dict[task_id]["elapsed_time"] = str(total_time)
            
            # ğŸ”¥ å¼‚å¸¸æ—¶ä¹Ÿè¦å‘é€å®Œæˆä¿¡å·
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            if world_size > 1:
                import torch.distributed as dist
                if dist.is_initialized():
                    try:
                        completion_signal = [{"type": "TASK_FAILED", "task_id": task_id, "error": str(e)}]
                        dist.broadcast_object_list(completion_signal, src=0)
                        logger.info(f"Task {task_id}: Failure signal sent to all ranks")
                    except Exception as broadcast_e:
                        logger.warning(f"Task {task_id}: Failed to send failure signal: {broadcast_e}")
            
            if task_id in cancelled_tasks:
                logger.info(f"âŒ Task {task_id} cancelled during processing")
            else:
                status_dict[task_id]["status"] = TaskStatus.FAILED
                status_dict[task_id]["error"] = str(e)
                status_dict[task_id]["updated_at"] = datetime.now().isoformat()
                logger.error(f"ğŸ’¥ Task {task_id} failed: {e}")

def task_processing_loop():
    """ç®€åŒ–çš„ä»»åŠ¡å¤„ç†å¾ªç¯"""
    logger.info("Task processing loop started")
    
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    while True:
        try:
            if task_queue:
                # ğŸ”¥ ç›´æ¥å¤„ç†ä»»åŠ¡ï¼Œä¸ç”¨é¢å¤–çš„å·¥ä½œç®¡ç†å™¨
                process_tasks()
            else:
                # ğŸ”¥ æ— ä»»åŠ¡æ—¶å‘é€ç©ºé—²ä¿¡å·
                if world_size > 1:
                    try:
                        import torch.distributed as dist
                        if dist.is_initialized():
                            idle_signal = [{"type": "IDLE"}]
                            dist.broadcast_object_list(idle_signal, src=0)
                            logger.debug("Idle signal sent")
                    except Exception as e:
                        logger.warning(f"Failed to send idle signal: {e}")
                
                time.sleep(1)  # ğŸ”¥ ç©ºé—²æ—¶ä¼‘çœ æ›´ä¹…
                
        except KeyboardInterrupt:
            logger.info("Task processing loop interrupted")
            
            # ğŸ”¥ å‘é€å…³é—­ä¿¡å·
            if world_size > 1:
                try:
                    import torch.distributed as dist
                    if dist.is_initialized():
                        shutdown_signal = [{"type": "SHUTDOWN"}]
                        dist.broadcast_object_list(shutdown_signal, src=0)
                        logger.info("Shutdown signal sent to all ranks")
                        
                        # ğŸ”¥ ç­‰å¾…æ‰€æœ‰rankç¡®è®¤å…³é—­
                        try:
                            dist.barrier(timeout=timedelta(seconds=5))
                            logger.info("All ranks confirmed shutdown")
                        except Exception as e:
                            logger.warning(f"Shutdown barrier failed: {e}")
                except Exception as e:
                    logger.warning(f"Failed to send shutdown signal: {e}")
            break
        except Exception as e:
            logger.error(f"Task processing error: {e}")
            time.sleep(1)

def distributed_worker_loop():
    """æ”¯æŒåŠ¨æ€è°ƒåº¦çš„å·¥ä½œå¾ªç¯"""
    rank = int(os.environ.get("RANK", 0))
    
    logger.info(f"Rank {rank}: Worker ready for dynamic scheduling")
    
    if rank == 0:
        return
    
    import torch.distributed as dist
    
    while True:
        try:
            if dist.is_initialized():
                signal = [None]
                dist.broadcast_object_list(signal, src=0)
                
                if signal[0] is not None:
                    signal_data = signal[0]
                    signal_type = signal_data.get("type", "TASK")
                    
                    if signal_type == "SHUTDOWN":
                        logger.info(f"Rank {rank}: Received shutdown signal")
                        break
                    elif signal_type == "IDLE":
                        continue
                    elif "request" in signal_data and "task_id" in signal_data:
                        request = signal_data['request']
                        task_id = signal_data['task_id']
                        
                        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æ˜¯åŠ¨æ€è°ƒåº¦æ¨¡å¼
                        if signal_data.get('dynamic_mode'):
                            assigned_ranks = signal_data.get('assigned_ranks', [])
                            
                            if rank in assigned_ranks:
                                logger.info(f"Rank {rank}: Participating in dynamic task {task_id}")
                                try:
                                    pipeline.generate_video(request, task_id)
                                    logger.info(f"Rank {rank}: Dynamic task {task_id} completed")
                                except Exception as e:
                                    logger.error(f"Rank {rank}: Dynamic task {task_id} failed: {e}")
                            else:
                                logger.info(f"Rank {rank}: Not assigned to task {task_id}, staying idle")
                                continue
                        else:
                            # ä¼ ç»Ÿå…¨GPUæ¨¡å¼
                            logger.info(f"Rank {rank}: Participating in full-GPU task {task_id}")
                            try:
                                pipeline.generate_video(request, task_id)
                                logger.info(f"Rank {rank}: Full-GPU task {task_id} completed")
                            except Exception as e:
                                logger.error(f"Rank {rank}: Full-GPU task {task_id} failed: {e}")
                        
                        # åŒæ­¥ç­‰å¾…
                        try:
                            dist.barrier(timeout=timedelta(seconds=30))
                        except Exception as e:
                            logger.warning(f"Rank {rank}: Barrier failed: {e}")
                
        except Exception as e:
            logger.warning(f"Rank {rank}: Worker error: {e}")
            time.sleep(1)
    
    logger.info(f"Rank {rank}: Dynamic worker exited")

def main():
    global pipeline, load_monitor, dynamic_scheduler
    rank, local_rank, world_size = init_distributed()
    
    pipeline = create_pipeline()
    logger.info(f"Rank {rank}: Pipeline created successfully")
    
    # ğŸ”¥ åˆå§‹åŒ–åŠ¨æ€è°ƒåº¦å™¨
    if rank == 0 and world_size > 1:
        enable_dynamic = os.environ.get("ENABLE_DYNAMIC_SCHEDULING", "true").lower() == "true"
        if enable_dynamic:
            from utils.device_detector import detect_device
            device_type, _, _ = detect_device()
            dynamic_scheduler = DynamicGPUScheduler(world_size=world_size, device_type=device_type)
            dynamic_scheduler.start()
            logger.info("ğŸ¯ Dynamic GPU scheduling enabled")
        else:
            logger.info("Dynamic GPU scheduling disabled")
    
    if rank == 0:
        app = create_app()
        
        # ğŸ”¥ åªå¯åŠ¨ä»»åŠ¡å¤„ç†çº¿ç¨‹
        task_thread = threading.Thread(target=task_processing_loop, daemon=True)
        task_thread.start()
        
        try:
            uvicorn.run(app, host="0.0.0.0", port=8088, log_level="info")
        except KeyboardInterrupt:
            logger.info("Shutting down gracefully...")
        finally:
            # ğŸ”¥ æ·»åŠ ï¼šå…³é—­åŠ¨æ€è°ƒåº¦å™¨
            if dynamic_scheduler:
                dynamic_scheduler.stop()
            logger.info("FastAPI server stopped")
    else:
        # ğŸ”¥ å…¶ä»–rankç›´æ¥è¿è¡Œå·¥ä½œå¾ªç¯
        try:
            distributed_worker_loop()
        except KeyboardInterrupt:
            logger.info(f"Rank {rank}: Received interrupt")
        finally:
            logger.info(f"Rank {rank}: Worker stopped")

if __name__ == "__main__":
    main()