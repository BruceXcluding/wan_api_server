import torch
import torch.distributed as dist
import logging
import time
from typing import List, Dict, Optional, Set
from dataclasses import dataclass
from .load_monitor import LoadMonitor

logger = logging.getLogger(__name__)

@dataclass
class TaskResourcePlan:
    """ä»»åŠ¡èµ„æºè®¡åˆ’"""
    task_id: str
    required_gpus: int
    assigned_ranks: List[int]
    complexity_level: str  # "simple", "medium", "complex"
    estimated_time: float

class DynamicGPUScheduler:
    """åŠ¨æ€GPUè°ƒåº¦å™¨"""
    
    def __init__(self, world_size: int, device_type: str = "npu"):
        self.world_size = world_size
        self.device_type = device_type
        self.load_monitor = LoadMonitor(world_size, device_type)
        
        # èµ„æºç®¡ç†
        self.available_ranks: Set[int] = set(range(world_size))
        self.active_tasks: Dict[str, TaskResourcePlan] = {}
        self.task_queue: List[TaskResourcePlan] = []
        
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.load_monitor.start()
        logger.info("Dynamic GPU scheduler started")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.load_monitor.stop()
        logger.info("Dynamic GPU scheduler stopped")
    

    def estimate_task_requirements(self, request) -> TaskResourcePlan:
        """ä¼°ç®—ä»»åŠ¡èµ„æºéœ€æ±‚"""

        # ğŸ”¥ è·å–åˆ†é…ç­–ç•¥
        import os
        strategy = os.environ.get("GPU_ALLOCATION_STRATEGY", "adaptive")

        # è§£æä»»åŠ¡å‚æ•°
        image_size = getattr(request, 'image_size', '512*512')
        frame_num = getattr(request, 'frame_num', 81)
        sample_steps = getattr(request, 'sample_steps', 30)

        # è§£æåˆ†è¾¨ç‡
        if '*' in image_size:
            h, w = map(int, image_size.split('*'))
        elif 'x' in image_size:
            w, h = map(int, image_size.split('x'))
        else:
            h, w = 512, 512

        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        total_pixels = h * w * frame_num
        complexity_score = total_pixels * sample_steps / 1000000

        # ğŸ”¥ æ ¹æ®é›†ç¾¤è§„æ¨¡åŠ¨æ€è®¡ç®—GPUåˆ†é…
        if strategy == "conservative":
            # ä¿å®ˆç­–ç•¥ï¼šæ›´å°‘GPUï¼Œæ›´å¤šå¹¶å‘
            if complexity_score < 800:
                required_gpus = max(1, self.world_size // 8)  # 1/8é›†ç¾¤å¤§å°ï¼Œæœ€å°‘1å¼ 
                complexity_level = "simple"
                estimated_time = 45
            elif complexity_score < 2000:
                required_gpus = max(1, self.world_size // 4)  # 1/4é›†ç¾¤å¤§å°
                complexity_level = "medium"
                estimated_time = 80
            else:
                required_gpus = max(2, self.world_size // 2)  # 1/2é›†ç¾¤å¤§å°ï¼Œæœ€å°‘2å¼ 
                complexity_level = "complex"
                estimated_time = 150

        elif strategy == "aggressive":
            # æ¿€è¿›ç­–ç•¥ï¼šæ›´å¤šGPUï¼Œæ›´å¿«å®Œæˆ
            if complexity_score < 1500:
                required_gpus = max(2, self.world_size // 2)  # 1/2é›†ç¾¤å¤§å°ï¼Œæœ€å°‘2å¼ 
                complexity_level = "simple"
                estimated_time = 20
            else:
                required_gpus = self.world_size  # å…¨éƒ¨GPU
                complexity_level = "complex"
                estimated_time = 90

        else:  # adaptiveï¼ˆé»˜è®¤ï¼‰
            # è‡ªé€‚åº”ç­–ç•¥ï¼šå¹³è¡¡GPUä½¿ç”¨å’Œå¹¶å‘
            if complexity_score < 800:
                required_gpus = max(1, self.world_size // 4)  # 1/4é›†ç¾¤å¤§å°ï¼Œæœ€å°‘1å¼ 
                complexity_level = "simple"
                estimated_time = 30
            elif complexity_score < 2500:
                required_gpus = max(2, self.world_size // 2)  # 1/2é›†ç¾¤å¤§å°ï¼Œæœ€å°‘2å¼ 
                complexity_level = "medium"
                estimated_time = 60
            else:
                required_gpus = self.world_size  # å…¨éƒ¨GPU
                complexity_level = "complex"
                estimated_time = 120

        # ğŸ”¥ ç¡®ä¿ä¸è¶…è¿‡é›†ç¾¤å¤§å°
        required_gpus = min(required_gpus, self.world_size)

        logger.info(f"Strategy: {strategy}, Cluster: {self.world_size} GPUs, Complexity: {complexity_score:.1f}, Allocated: {required_gpus} GPUs")

        return TaskResourcePlan(
            task_id="",
            required_gpus=required_gpus,
            assigned_ranks=[],
            complexity_level=complexity_level,
            estimated_time=estimated_time
        )

    def try_allocate_gpus(self, plan: TaskResourcePlan) -> Optional[List[int]]:
        """å°è¯•åˆ†é…GPU"""
        
        if len(self.available_ranks) < plan.required_gpus:
            logger.info(f"Insufficient GPUs: need {plan.required_gpus}, available {len(self.available_ranks)}")
            return None
        
        # ğŸ”¥ é€‰æ‹©è´Ÿè½½æœ€ä½çš„GPU
        load_status = self.load_monitor.get_load_status()
        available_in_status = [r for r in load_status["available"] if r in self.available_ranks]
        busy_in_status = [r for r in load_status["busy"] if r in self.available_ranks]
        
        # ä¼˜å…ˆä½¿ç”¨ç©ºé—²GPU
        candidate_ranks = available_in_status + busy_in_status
        
        if len(candidate_ranks) >= plan.required_gpus:
            selected_ranks = candidate_ranks[:plan.required_gpus]
            
            # ğŸ”¥ åˆ†é…GPU
            for rank in selected_ranks:
                self.available_ranks.remove(rank)
            
            plan.assigned_ranks = selected_ranks
            self.active_tasks[plan.task_id] = plan
            
            logger.info(f"Task {plan.task_id}: Allocated {plan.required_gpus} GPUs: {selected_ranks}")
            return selected_ranks
        
        return None
    
    def release_gpus(self, task_id: str):
        """é‡Šæ”¾GPUèµ„æº"""
        if task_id in self.active_tasks:
            plan = self.active_tasks.pop(task_id)
            
            # é‡Šæ”¾GPU
            for rank in plan.assigned_ranks:
                self.available_ranks.add(rank)
            
            logger.info(f"Task {task_id}: Released GPUs {plan.assigned_ranks}")
            
            # ğŸ”¥ å°è¯•å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
            self._try_start_queued_tasks()
    
    def _try_start_queued_tasks(self):
        """å°è¯•å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
        for plan in self.task_queue[:]:
            if self.try_allocate_gpus(plan):
                self.task_queue.remove(plan)
                logger.info(f"Started queued task {plan.task_id}")
    
    def schedule_task(self, task_id: str, request) -> Optional[List[int]]:
        """è°ƒåº¦ä»»åŠ¡"""
        
        # åˆ›å»ºèµ„æºè®¡åˆ’
        plan = self.estimate_task_requirements(request)
        plan.task_id = task_id
        
        logger.info(f"Task {task_id}: {plan.complexity_level} task, requires {plan.required_gpus} GPUs")
        
        # å°è¯•ç«‹å³åˆ†é…
        assigned_ranks = self.try_allocate_gpus(plan)
        
        if assigned_ranks:
            return assigned_ranks
        else:
            # åŠ å…¥é˜Ÿåˆ—
            self.task_queue.append(plan)
            logger.info(f"Task {task_id}: Queued, waiting for {plan.required_gpus} GPUs")
            return None
    
    def get_scheduler_status(self) -> Dict:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        return {
            "total_gpus": self.world_size,
            "available_gpus": len(self.available_ranks),
            "active_tasks": len(self.active_tasks),
            "queued_tasks": len(self.task_queue),
            "available_ranks": list(self.available_ranks),
            "active_task_details": {
                task_id: {
                    "assigned_ranks": plan.assigned_ranks,
                    "complexity": plan.complexity_level,
                    "required_gpus": plan.required_gpus
                }
                for task_id, plan in self.active_tasks.items()
            }
        }