import torch
import torch.distributed as dist
import logging
import time
import threading
from typing import List, Dict, Optional, Set
from dataclasses import dataclass
from .load_monitor import LoadMonitor

logger = logging.getLogger(__name__)

@dataclass
class TaskResourcePlan:
    """ä»»åŠ¡èµ„æºè®¡åˆ’"""
    task_id: str
    required_gpus: int
    assigned_ranks: List[int]
    complexity_level: str  # "simple", "medium", "complex"
    estimated_time: float

class DynamicGPUScheduler:
    """åŠ¨æ€GPUè°ƒåº¦å™¨"""
    
    def __init__(self, world_size: int, device_type: str = "npu"):
        self.world_size = world_size
        self.device_type = device_type
        self.load_monitor = LoadMonitor(world_size, device_type)
        
        # èµ„æºç®¡ç†
        self.available_ranks: Set[int] = set(range(world_size))
        self.active_tasks: Dict[str, TaskResourcePlan] = {}
        self.task_queue: List[TaskResourcePlan] = []
        self.task_lock = threading.Lock()
        self.gpu_status = {i: "available" for i in range(world_size)}
        self.pending_tasks = {}
        self.running = False
        
        logger.info(f"ğŸ¯ Dynamic GPU Scheduler initialized: {world_size} {device_type.upper()} devices")

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.load_monitor.start()
        logger.info("Dynamic GPU scheduler started")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.load_monitor.stop()
        logger.info("Dynamic GPU scheduler stopped")
    

    def estimate_task_requirements(self, request) -> TaskResourcePlan:
        """ä¼°ç®—ä»»åŠ¡èµ„æºéœ€æ±‚"""

        # ğŸ”¥ è·å–åˆ†é…ç­–ç•¥
        import os
        strategy = os.environ.get("GPU_ALLOCATION_STRATEGY", "adaptive")

        # è§£æä»»åŠ¡å‚æ•°
        image_size = getattr(request, 'image_size', '512*512')
        frame_num = getattr(request, 'frame_num', 81)
        sample_steps = getattr(request, 'sample_steps', 30)

        # è§£æåˆ†è¾¨ç‡
        if '*' in image_size:
            h, w = map(int, image_size.split('*'))
        elif 'x' in image_size:
            w, h = map(int, image_size.split('x'))
        else:
            h, w = 512, 512

        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        total_pixels = h * w * frame_num
        complexity_score = total_pixels * sample_steps / 1000000

        # ğŸ”¥ æ ¹æ®é›†ç¾¤è§„æ¨¡åŠ¨æ€è®¡ç®—GPUåˆ†é…
        if strategy == "conservative":
            # ä¿å®ˆç­–ç•¥ï¼šæ›´å°‘GPUï¼Œæ›´å¤šå¹¶å‘
            if complexity_score < 800:
                required_gpus = max(1, self.world_size // 8)  # 1/8é›†ç¾¤å¤§å°ï¼Œæœ€å°‘1å¼ 
                complexity_level = "simple"
                estimated_time = 45
            elif complexity_score < 2000:
                required_gpus = max(1, self.world_size // 4)  # 1/4é›†ç¾¤å¤§å°
                complexity_level = "medium"
                estimated_time = 80
            else:
                required_gpus = max(2, self.world_size // 2)  # 1/2é›†ç¾¤å¤§å°ï¼Œæœ€å°‘2å¼ 
                complexity_level = "complex"
                estimated_time = 150

        elif strategy == "aggressive":
            # æ¿€è¿›ç­–ç•¥ï¼šæ›´å¤šGPUï¼Œæ›´å¿«å®Œæˆ
            if complexity_score < 1500:
                required_gpus = max(2, self.world_size // 2)  # 1/2é›†ç¾¤å¤§å°ï¼Œæœ€å°‘2å¼ 
                complexity_level = "simple"
                estimated_time = 20
            else:
                required_gpus = self.world_size  # å…¨éƒ¨GPU
                complexity_level = "complex"
                estimated_time = 90

        else:  # adaptiveï¼ˆé»˜è®¤ï¼‰
            # è‡ªé€‚åº”ç­–ç•¥ï¼šå¹³è¡¡GPUä½¿ç”¨å’Œå¹¶å‘
            if complexity_score < 800:
                required_gpus = max(1, self.world_size // 4)  # 1/4é›†ç¾¤å¤§å°ï¼Œæœ€å°‘1å¼ 
                complexity_level = "simple"
                estimated_time = 30
            elif complexity_score < 2500:
                required_gpus = max(2, self.world_size // 2)  # 1/2é›†ç¾¤å¤§å°ï¼Œæœ€å°‘2å¼ 
                complexity_level = "medium"
                estimated_time = 60
            else:
                required_gpus = self.world_size  # å…¨éƒ¨GPU
                complexity_level = "complex"
                estimated_time = 120

        # ğŸ”¥ ç¡®ä¿ä¸è¶…è¿‡é›†ç¾¤å¤§å°
        required_gpus = min(required_gpus, self.world_size)

        logger.info(f"Strategy: {strategy}, Cluster: {self.world_size} GPUs, Complexity: {complexity_score:.1f}, Allocated: {required_gpus} GPUs")

        return TaskResourcePlan(
            task_id="",
            required_gpus=required_gpus,
            assigned_ranks=[],
            complexity_level=complexity_level,
            estimated_time=estimated_time
        )

    def try_allocate_gpus(self, plan: TaskResourcePlan) -> Optional[List[int]]:
        """å°è¯•åˆ†é…GPU"""
        
        if len(self.available_ranks) < plan.required_gpus:
            logger.info(f"Insufficient GPUs: need {plan.required_gpus}, available {len(self.available_ranks)}")
            return None
        
        # ğŸ”¥ é€‰æ‹©è´Ÿè½½æœ€ä½çš„GPU
        load_status = self.load_monitor.get_load_status()
        available_in_status = [r for r in load_status["available"] if r in self.available_ranks]
        busy_in_status = [r for r in load_status["busy"] if r in self.available_ranks]
        
        # ä¼˜å…ˆä½¿ç”¨ç©ºé—²GPU
        candidate_ranks = available_in_status + busy_in_status
        
        if len(candidate_ranks) >= plan.required_gpus:
            selected_ranks = candidate_ranks[:plan.required_gpus]
            
            # ğŸ”¥ åˆ†é…GPU
            for rank in selected_ranks:
                self.available_ranks.remove(rank)
            
            plan.assigned_ranks = selected_ranks
            self.active_tasks[plan.task_id] = plan
            
            logger.info(f"Task {plan.task_id}: Allocated {plan.required_gpus} GPUs: {selected_ranks}")
            return selected_ranks
        
        return None

    def find_available_gpus(self, required_count: int) -> Optional[List[int]]:
        """æŸ¥æ‰¾å¯ç”¨çš„GPU"""
        available_list = [rank for rank, status in self.gpu_status.items() if status == "available"]

        if len(available_list) >= required_count:
            return available_list[:required_count]
        else:
            return None


    def release_gpus(self, task_id: str):
        """é‡Šæ”¾GPUèµ„æº"""
        if task_id in self.active_tasks:
            plan = self.active_tasks.pop(task_id)

            # ğŸ”¥ æ›´æ–°ä¸¤ç§çŠ¶æ€è®°å½•
            for rank in plan.assigned_ranks:
                self.gpu_status[rank] = "available"
                self.available_ranks.add(rank)

            logger.info(f"Task {task_id}: Released GPUs {plan.assigned_ranks}")

            # ğŸ”¥ å°è¯•å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
            self._try_start_queued_tasks()

            # ğŸ”¥ æ¸…ç†ç­‰å¾…é˜Ÿåˆ—ä¸­çš„å·²å®Œæˆä»»åŠ¡
            if task_id in self.pending_tasks:
                del self.pending_tasks[task_id]

    def _try_start_queued_tasks(self):
        """å°è¯•å¯åŠ¨é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
        scheduled_tasks = []

        for plan in self.task_queue[:]:  # å¤åˆ¶åˆ—è¡¨é¿å…ä¿®æ”¹æ—¶å‡ºé”™
            available_ranks = self.find_available_gpus(plan.required_gpus)

            if available_ranks is not None:
                # åˆ†é…GPU
                plan.assigned_ranks = available_ranks
                self.active_tasks[plan.task_id] = plan

                # æ›´æ–°çŠ¶æ€
                for rank in available_ranks:
                    self.gpu_status[rank] = "busy"
                    self.available_ranks.discard(rank)

                scheduled_tasks.append(plan)
                logger.info(f"Started queued task {plan.task_id} with GPUs {available_ranks}")

        # ç§»é™¤å·²è°ƒåº¦çš„ä»»åŠ¡
        for plan in scheduled_tasks:
            self.task_queue.remove(plan)
            if plan.task_id in self.pending_tasks:
                del self.pending_tasks[plan.task_id]

    def schedule_task(self, task_id: str, request) -> Optional[List[int]]:
        """è°ƒåº¦ä»»åŠ¡åˆ°æœ€ä½³GPUç»„åˆ"""
        with self.task_lock:
            # ä¼°ç®—èµ„æºéœ€æ±‚
            plan = self.estimate_task_requirements(request)
            plan.task_id = task_id

            logger.info(f"ğŸ¯ Task {task_id}: Requires {plan.required_gpus} GPUs ({plan.complexity_level})")

            # æŸ¥æ‰¾å¯ç”¨GPU
            available_ranks = self.find_available_gpus(plan.required_gpus)

            if available_ranks is None:
                # æ²¡æœ‰è¶³å¤Ÿçš„GPUï¼ŒåŠ å…¥ç­‰å¾…é˜Ÿåˆ—
                self.pending_tasks[task_id] = plan
                self.task_queue.append(plan)  # ğŸ”¥ åŒæ—¶æ·»åŠ åˆ°ä¸¤ä¸ªé˜Ÿåˆ—ä¿æŒå…¼å®¹
                logger.warning(f"â³ Task {task_id}: Insufficient GPUs, queued for later (needs {plan.required_gpus})")
                return None

            # åˆ†é…GPU
            plan.assigned_ranks = available_ranks
            self.active_tasks[task_id] = plan

            # ğŸ”¥ æ›´æ–°ä¸¤ç§çŠ¶æ€è®°å½•
            for rank in available_ranks:
                self.gpu_status[rank] = "busy"
                self.available_ranks.discard(rank)  # ä»å¯ç”¨é›†åˆä¸­ç§»é™¤

            logger.info(f"âœ… Task {task_id}: Allocated {len(available_ranks)} GPUs: {available_ranks}")
            return available_ranks

    def get_scheduler_status(self) -> Dict:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        with self.task_lock:
            available_gpus = [rank for rank, status in self.gpu_status.items() if status == "available"]
            busy_gpus = [rank for rank, status in self.gpu_status.items() if status == "busy"]
            
            return {
                "dynamic_scheduling": True,
                "world_size": self.world_size,
                "device_type": self.device_type,
                "total_gpus": self.world_size,
                "available_gpus": len(available_gpus),
                "busy_gpus": len(busy_gpus),
                "active_tasks": len(self.active_tasks),
                "queued_tasks": len(self.task_queue),
                "pending_tasks": len(self.pending_tasks),
                "available_ranks": available_gpus,
                "busy_ranks": busy_gpus,
                "running": getattr(self, 'running', False),
                "gpu_status": dict(self.gpu_status),
                "active_task_details": {
                    task_id: {
                        "assigned_ranks": plan.assigned_ranks,
                        "complexity": plan.complexity_level,
                        "required_gpus": plan.required_gpus,
                        "estimated_time": plan.estimated_time
                    }
                    for task_id, plan in self.active_tasks.items()
                }
            }