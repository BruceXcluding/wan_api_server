import os
import sys
import torch
import torch.distributed as dist
from datetime import datetime, timedelta

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)

import logging
import uuid
import threading
import time
from typing import List
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import uvicorn

from schemas import VideoSubmitRequest, VideoSubmitResponse, VideoStatusResponse, TaskStatus
from pipelines.pipeline_factory import create_pipeline
from utils.device_detector import detect_device

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
task_queue = []
status_dict = {}
cancelled_tasks = set()
pipeline = None

def init_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    try:
        import torch_npu
        npu_available = torch_npu.npu.is_available()
    except ImportError:
        npu_available = False
    
    # è·å–åˆ†å¸ƒå¼ä¿¡æ¯
    rank = int(os.environ.get("RANK", 0))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    logger.info(f"Initializing rank {rank}/{world_size}, local_rank {local_rank}")
    
    if world_size > 1:
        # ğŸ”¥ å…ˆè®¾ç½®è®¾å¤‡å†åˆå§‹åŒ–åˆ†å¸ƒå¼
        if npu_available:
            import torch_npu
            torch_npu.npu.set_device(local_rank)  # ğŸ”¥ ä½¿ç”¨torch_npu.npu.set_device
            logger.info(f"Rank {rank}: Set NPU device {local_rank}")
        elif torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
            logger.info(f"Rank {rank}: Set CUDA device {local_rank}")
        
        # ğŸ”¥ ä½¿ç”¨æ­£ç¡®çš„åˆ†å¸ƒå¼åˆå§‹åŒ–
        backend = "hccl" if npu_available else "nccl" if torch.cuda.is_available() else "gloo"
        
        try:
            if not dist.is_initialized():
                dist.init_process_group(
                    backend=backend,
                    timeout=timedelta(seconds=300),  # ğŸ”¥ æ·»åŠ è¶…æ—¶
                    init_method='env://'  # ğŸ”¥ æ˜ç¡®æŒ‡å®š
                )
                logger.info(f"Rank {rank}: Distributed initialized with {backend}")
            else:
                logger.info(f"Rank {rank}: Distributed already initialized")
        except Exception as e:
            logger.error(f"Rank {rank}: Failed to initialize distributed: {e}")
            raise
    else:
        logger.info(f"Rank {rank}: Single device mode")
    
    return rank, local_rank, world_size

def create_app():
    """åˆ›å»ºFastAPIåº”ç”¨"""
    app = FastAPI(title="Wan2.1 I2V Multi-Device API Server")
    app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])
    
    os.makedirs("generated_videos", exist_ok=True)
    app.mount("/videos", StaticFiles(directory="generated_videos"), name="videos")

    @app.post("/submit", response_model=VideoSubmitResponse)
    async def submit(request: VideoSubmitRequest):
        task_id = f"req_{uuid.uuid4().hex[:16]}"
        status_dict[task_id] = {
            "status": TaskStatus.PENDING, 
            "result_url": "", 
            "error": "",
            "created_at": datetime.now().isoformat(),
            "progress": 0
        }
        task_queue.append((task_id, request))
        logger.info(f"Task submitted: {task_id}")
        return VideoSubmitResponse(requestId=task_id, status=TaskStatus.PENDING, message="ä»»åŠ¡å·²æäº¤", estimated_time=30)

    @app.post("/batch_submit", response_model=List[VideoSubmitResponse])
    async def batch_submit(requests: List[VideoSubmitRequest]):
        responses = []
        for req in requests:
            task_id = f"req_{uuid.uuid4().hex[:16]}"
            status_dict[task_id] = {
                "status": TaskStatus.PENDING, 
                "result_url": "", 
                "error": "",
                "created_at": datetime.now().isoformat(),
                "progress": 0
            }
            task_queue.append((task_id, req))
            responses.append(VideoSubmitResponse(requestId=task_id, status=TaskStatus.PENDING, message="ä»»åŠ¡å·²æäº¤", estimated_time=30))
        return responses

    # ğŸ”¥ æ–°å¢ï¼šå–æ¶ˆä»»åŠ¡
    @app.post("/cancel/{task_id}")
    async def cancel_task(task_id: str):
        if task_id not in status_dict:
            raise HTTPException(status_code=404, detail="Task not found")
        
        current_status = status_dict[task_id]["status"]
        if current_status in [TaskStatus.SUCCEED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
            raise HTTPException(status_code=400, detail=f"Task already {current_status.value}")
        
        cancelled_tasks.add(task_id)
        status_dict[task_id]["status"] = TaskStatus.CANCELLED
        status_dict[task_id]["updated_at"] = datetime.now().isoformat()
        
        # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
        global task_queue
        task_queue = [(tid, req) for tid, req in task_queue if tid != task_id]
        
        logger.info(f"Task {task_id} cancelled")
        return {"message": f"Task {task_id} cancelled successfully"}

    @app.post("/cancel_all")
    async def cancel_all():
        cancelled_count = 0
        for task_id, task_info in status_dict.items():
            if task_info["status"] in [TaskStatus.PENDING, TaskStatus.RUNNING]:
                cancelled_tasks.add(task_id)
                status_dict[task_id]["status"] = TaskStatus.CANCELLED
                status_dict[task_id]["updated_at"] = datetime.now().isoformat()
                cancelled_count += 1
        
        global task_queue
        task_queue = []
        
        logger.info(f"Cancelled {cancelled_count} tasks")
        return {"message": f"Cancelled {cancelled_count} tasks"}

    @app.get("/status/{task_id}", response_model=VideoStatusResponse)
    async def status(task_id: str):
        s = status_dict.get(task_id)
        if not s:
            raise HTTPException(status_code=404, detail="Task not found")

        # ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€æ¶ˆæ¯
        message = "ä»»åŠ¡å¤„ç†ä¸­"
        if s.get("status") == TaskStatus.RUNNING:
            stage = s.get("current_stage", "å¤„ç†ä¸­")
            progress = s.get("progress", 0)
            message = f"{stage} ({progress:.1f}%)"
        elif s.get("status") == TaskStatus.FAILED:
            message = s.get("error", "ç”Ÿæˆå¤±è´¥")
        elif s.get("status") == TaskStatus.SUCCEED:
            message = "ç”Ÿæˆå®Œæˆ"
        elif s.get("status") == TaskStatus.CANCELLED:
            message = "ä»»åŠ¡å·²å–æ¶ˆ"

        return VideoStatusResponse(
            requestId=task_id,
            status=s.get("status", TaskStatus.PENDING),
            progress=s.get("progress", 0),
            message=message,  # ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨åŠ¨æ€æ¶ˆæ¯
            created_at=s.get("created_at", ""),
            updated_at=s.get("updated_at", ""),
            results=s.get("results"),
            reason=s.get("reason"),
            elapsed_time=s.get("elapsed_time"),
            current_stage=s.get("current_stage"),  # ğŸ”¥ æ–°å¢
        )

    @app.get("/health")
    async def health():
        dtype, dcount, backend = detect_device()
        return {
            "device_type": dtype, 
            "device_count": dcount, 
            "backend": backend, 
            "queue_size": len(task_queue), 
            "total_tasks": len(status_dict),
            "cancelled_tasks": len(cancelled_tasks)
        }
    
    @app.get("/monitor")
    async def monitor():
        """ç®€å•çš„ä»»åŠ¡ç›‘æ§"""
        running = [{"id": k, "progress": v.get("progress", 0), "stage": v.get("current_stage", "")} 
                   for k, v in status_dict.items() if v.get("status") == TaskStatus.RUNNING]

        return {
            "queue_size": len(task_queue),
            "running_tasks": running,
            "total_tasks": len(status_dict)
        }

    return app

def process_tasks():
    """å¤„ç†ä»»åŠ¡é˜Ÿåˆ—"""
    while task_queue:
        task_id, request = task_queue.pop(0)
        
        if task_id in cancelled_tasks:
            logger.info(f"Task {task_id} was cancelled, skipping")
            continue
            
        try:
            logger.info(f"ğŸš€ Processing task: {task_id}")
            status_dict[task_id]["status"] = TaskStatus.RUNNING
            status_dict[task_id]["updated_at"] = datetime.now().isoformat()
            status_dict[task_id]["start_time"] = datetime.now()
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¹¿æ’­ä»»åŠ¡ç»™æ‰€æœ‰rank
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            if world_size > 1:
                import torch.distributed as dist
                if dist.is_initialized():
                    task_data = [{'request': request, 'task_id': task_id}]
                    dist.broadcast_object_list(task_data, src=0)
                    logger.info(f"Task {task_id} broadcasted to all {world_size} ranks")
            
            def progress_callback(progress, stage="Processing"):
                if task_id in cancelled_tasks:
                    raise Exception("Task was cancelled")
                status_dict[task_id]["progress"] = progress
                status_dict[task_id]["current_stage"] = stage
                
                elapsed = datetime.now() - status_dict[task_id]["start_time"]
                logger.info(f"ğŸ“Š Task {task_id}: {stage} ({progress:.1f}%) - Elapsed: {elapsed}")
                return progress
            
            # ğŸ”¥ ç°åœ¨æ‰€æœ‰rankéƒ½ä¼šå‚ä¸è¿™ä¸ªè°ƒç”¨
            result = pipeline.generate_video(request, task_id, progress_callback=progress_callback)
            
            # ğŸ”¥ æ·»åŠ ï¼šä»»åŠ¡å®Œæˆåç­‰å¾…æ‰€æœ‰rankåŒæ­¥
            if world_size > 1:
                import torch.distributed as dist
                if dist.is_initialized():
                    try:
                        dist.barrier(timeout=timedelta(seconds=30))
                        logger.info(f"Task {task_id}: All ranks synchronized after completion")
                    except Exception as e:
                        logger.warning(f"Task {task_id}: Post-completion barrier failed: {e}")
                    
                    # ğŸ”¥ æ–°æ·»åŠ ï¼šå‘é€å®Œæˆä¿¡å·ï¼Œè®©å…¶ä»–rankç»“æŸå½“å‰ä»»åŠ¡ç­‰å¾…
                    try:
                        completion_signal = [{"type": "TASK_COMPLETED", "task_id": task_id}]
                        dist.broadcast_object_list(completion_signal, src=0)
                        logger.info(f"Task {task_id}: Completion signal sent to all ranks")
                    except Exception as e:
                        logger.warning(f"Task {task_id}: Failed to send completion signal: {e}")
             
            # æœ€åæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦è¢«å–æ¶ˆ
            if task_id in cancelled_tasks:
                logger.info(f"Task {task_id} was cancelled during processing")
                continue
            
            # ğŸ”¥ æ–°å¢ï¼šè®¡ç®—æ€»è€—æ—¶
            total_time = datetime.now() - status_dict[task_id]["start_time"]
            
            status_dict[task_id]["status"] = TaskStatus.SUCCEED
            status_dict[task_id]["result_url"] = result
            status_dict[task_id]["updated_at"] = datetime.now().isoformat()
            status_dict[task_id]["elapsed_time"] = str(total_time)  # ğŸ”¥ æ–°å¢
            logger.info(f"âœ… Task {task_id} completed successfully in {total_time}")
            
        except Exception as e:
            # ğŸ”¥ æ–°å¢ï¼šå¼‚å¸¸æ—¶ä¹Ÿè®°å½•è€—æ—¶å’Œå‘é€å®Œæˆä¿¡å·
            if "start_time" in status_dict[task_id]:
                total_time = datetime.now() - status_dict[task_id]["start_time"]
                status_dict[task_id]["elapsed_time"] = str(total_time)
            
            # ğŸ”¥ å¼‚å¸¸æ—¶ä¹Ÿè¦å‘é€å®Œæˆä¿¡å·
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            if world_size > 1:
                import torch.distributed as dist
                if dist.is_initialized():
                    try:
                        completion_signal = [{"type": "TASK_FAILED", "task_id": task_id, "error": str(e)}]
                        dist.broadcast_object_list(completion_signal, src=0)
                        logger.info(f"Task {task_id}: Failure signal sent to all ranks")
                    except Exception as broadcast_e:
                        logger.warning(f"Task {task_id}: Failed to send failure signal: {broadcast_e}")
            
            if task_id in cancelled_tasks:
                logger.info(f"âŒ Task {task_id} cancelled during processing")
            else:
                status_dict[task_id]["status"] = TaskStatus.FAILED
                status_dict[task_id]["error"] = str(e)
                status_dict[task_id]["updated_at"] = datetime.now().isoformat()
                logger.error(f"ğŸ’¥ Task {task_id} failed: {e}")

def task_processing_loop():
    """ç®€åŒ–çš„ä»»åŠ¡å¤„ç†å¾ªç¯"""
    logger.info("Task processing loop started")
    
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    while True:
        try:
            if task_queue:
                # ğŸ”¥ ç›´æ¥å¤„ç†ä»»åŠ¡ï¼Œä¸ç”¨é¢å¤–çš„å·¥ä½œç®¡ç†å™¨
                process_tasks()
            else:
                # ğŸ”¥ æ— ä»»åŠ¡æ—¶å‘é€ç©ºé—²ä¿¡å·
                if world_size > 1:
                    try:
                        import torch.distributed as dist
                        if dist.is_initialized():
                            idle_signal = [{"type": "IDLE"}]
                            dist.broadcast_object_list(idle_signal, src=0)
                            logger.debug("Idle signal sent")
                    except Exception as e:
                        logger.warning(f"Failed to send idle signal: {e}")
                
                time.sleep(1)  # ğŸ”¥ ç©ºé—²æ—¶ä¼‘çœ æ›´ä¹…
                
        except KeyboardInterrupt:
            logger.info("Task processing loop interrupted")
            
            # ğŸ”¥ å‘é€å…³é—­ä¿¡å·
            if world_size > 1:
                try:
                    import torch.distributed as dist
                    if dist.is_initialized():
                        shutdown_signal = [{"type": "SHUTDOWN"}]
                        dist.broadcast_object_list(shutdown_signal, src=0)
                        logger.info("Shutdown signal sent to all ranks")
                        
                        # ğŸ”¥ ç­‰å¾…æ‰€æœ‰rankç¡®è®¤å…³é—­
                        try:
                            dist.barrier(timeout=timedelta(seconds=5))
                            logger.info("All ranks confirmed shutdown")
                        except Exception as e:
                            logger.warning(f"Shutdown barrier failed: {e}")
                except Exception as e:
                    logger.warning(f"Failed to send shutdown signal: {e}")
            break
        except Exception as e:
            logger.error(f"Task processing error: {e}")
            time.sleep(1)

def distributed_worker_loop():
    """ç®€åŒ–çš„åˆ†å¸ƒå¼å·¥ä½œå¾ªç¯"""
    rank = int(os.environ.get("RANK", 0))
    
    logger.info(f"Rank {rank}: Worker ready for distributed tasks")
    
    if rank == 0:
        logger.info("Rank 0: Main worker loop handled by task processing")
        return
    
    import torch.distributed as dist
    
    while True:
        try:
            if dist.is_initialized():
                # ğŸ”¥ ç­‰å¾…rank 0çš„ä¿¡å·
                signal = [None]
                
                try:
                    dist.broadcast_object_list(signal, src=0)
                    
                    if signal[0] is not None:
                        signal_type = signal[0].get("type", "TASK")
                        
                        if signal_type == "SHUTDOWN":
                            logger.info(f"Rank {rank}: Received shutdown signal")
                            
                            # ğŸ”¥ ç¡®è®¤å…³é—­
                            try:
                                dist.barrier(timeout=timedelta(seconds=5))
                                logger.info(f"Rank {rank}: Shutdown confirmed")
                            except Exception as e:
                                logger.warning(f"Rank {rank}: Shutdown barrier failed: {e}")
                            break
                            
                        elif signal_type == "IDLE":
                            logger.debug(f"Rank {rank}: Server idle, continuing to wait...")
                            continue
                            
                        elif signal_type in ["TASK_COMPLETED", "TASK_FAILED"]:
                            logger.debug(f"Rank {rank}: Task completion signal received")
                            continue
                            
                        elif "request" in signal[0] and "task_id" in signal[0]:
                            # ğŸ”¥ è¿™æ˜¯çœŸæ­£çš„ä»»åŠ¡
                            request, task_id = signal[0]['request'], signal[0]['task_id']
                            logger.info(f"Rank {rank}: Received task {task_id}")
                            
                            try:
                                # ğŸ”¥ æ‰§è¡Œä»»åŠ¡
                                pipeline.generate_video(request, task_id)
                                logger.info(f"Rank {rank}: Task {task_id} completed successfully")
                                
                            except Exception as e:
                                logger.error(f"Rank {rank}: Task {task_id} failed: {e}")
                                
                            # ğŸ”¥ ä»»åŠ¡å®ŒæˆååŒæ­¥
                            try:
                                dist.barrier(timeout=timedelta(seconds=30))
                                logger.debug(f"Rank {rank}: Post-task barrier completed")
                            except Exception as e:
                                logger.warning(f"Rank {rank}: Post-task barrier failed: {e}")
                            
                            # ğŸ”¥ ç­‰å¾…å®Œæˆç¡®è®¤
                            try:
                                completion_signal = [None]
                                dist.broadcast_object_list(completion_signal, src=0)
                                logger.debug(f"Rank {rank}: Completion signal received")
                            except Exception as e:
                                logger.warning(f"Rank {rank}: Failed to receive completion signal: {e}")
                        else:
                            logger.debug(f"Rank {rank}: Unknown signal type: {signal_type}")
                    else:
                        logger.debug(f"Rank {rank}: Received empty signal")
                        
                except Exception as e:
                    # ğŸ”¥ broadcastå¤±è´¥ï¼ŒçŸ­æš‚ç­‰å¾…åé‡è¯•
                    logger.warning(f"Rank {rank}: Broadcast failed: {e}")
                    time.sleep(1)
                    
            else:
                logger.warning(f"Rank {rank}: Distributed not initialized")
                time.sleep(1)
                
        except KeyboardInterrupt:
            logger.info(f"Rank {rank}: Received interrupt")
            break
        except Exception as e:
            logger.error(f"Rank {rank}: Worker error: {e}")
            time.sleep(1)
    
    logger.info(f"Rank {rank}: Worker exited gracefully")

def main():
    global pipeline
    rank, local_rank, world_size = init_distributed()
    
    pipeline = create_pipeline()
    logger.info(f"Rank {rank}: Pipeline created successfully")
    
    if rank == 0:
        app = create_app()
        
        # ğŸ”¥ åªå¯åŠ¨ä»»åŠ¡å¤„ç†çº¿ç¨‹
        task_thread = threading.Thread(target=task_processing_loop, daemon=True)
        task_thread.start()
        
        try:
            uvicorn.run(app, host="0.0.0.0", port=8088, log_level="info")
        except KeyboardInterrupt:
            logger.info("Shutting down gracefully...")
        finally:
            logger.info("FastAPI server stopped")
    else:
        # ğŸ”¥ å…¶ä»–rankç›´æ¥è¿è¡Œå·¥ä½œå¾ªç¯
        try:
            distributed_worker_loop()
        except KeyboardInterrupt:
            logger.info(f"Rank {rank}: Received interrupt")
        finally:
            logger.info(f"Rank {rank}: Worker stopped")

if __name__ == "__main__":
    main()