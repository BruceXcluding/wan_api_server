import abc
import os
import logging
import torch
import torch.distributed as dist
from pathlib import Path
from typing import Optional, Callable, Dict, Any, List
from PIL import Image
import requests
from io import BytesIO

logger = logging.getLogger(__name__)

class BasePipeline(abc.ABC):
    """åˆ†å¸ƒå¼æ¨ç†åŸºç¡€ç®¡é“"""

    def __init__(self, ckpt_dir: str, **model_args):
        self.ckpt_dir = ckpt_dir
        self.model_args = model_args
        self.model = None
        self.device_type = model_args.get("device_type", "cuda")
        self.rank = int(os.environ.get("RANK", 0))
        self.world_size = int(os.environ.get("WORLD_SIZE", 1))
        self.local_rank = int(os.environ.get("LOCAL_RANK", 0))
        self.t5_cpu = model_args.get('t5_cpu', False)
        logger.info(f"Initializing {self.__class__.__name__} on rank {self.rank}")

        self.ulysses_size = model_args.get('ulysses_size', self.world_size)  # é»˜è®¤ç­‰äºworld_size
        self.ring_size = model_args.get('ring_size', 1)                      # é»˜è®¤ä¸ä½¿ç”¨ringå¹¶è¡Œ
        self.cfg_size = model_args.get('cfg_size', 1)                       # é»˜è®¤ä¸ä½¿ç”¨cfgå¹¶è¡Œ
    
        logger.info(f"Initializing {self.__class__.__name__} on rank {self.rank}")
        logger.info(f"Rank {self.rank}: Distributed config - ulysses_size={self.ulysses_size}, ring_size={self.ring_size}")
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè‡ªåŠ¨åˆå§‹åŒ–åˆ†å¸ƒå¼å’Œæ¨¡å‹
        self.initialize()

    def initialize(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼å’Œæ¨¡å‹"""
        # ğŸ”¥ ä¿®æ”¹ï¼šåªåœ¨å¤šå¡æ—¶åˆå§‹åŒ–åˆ†å¸ƒå¼
        if self.world_size > 1:
            self._init_distributed()
            # ğŸ”¥ æ–°å¢ï¼šè®¾å¤‡ç‰¹å®šçš„åˆ†å¸ƒå¼åˆå§‹åŒ–ï¼ˆé»˜è®¤ä¸ºç©ºï¼‰
            self._setup_device_distributed()
        self.model = self._load_model()
        logger.info(f"Rank {self.rank}: {self.device_type} Pipeline initialized successfully")
    
    def _setup_device_distributed(self):
        """è®¾å¤‡ç‰¹å®šçš„åˆ†å¸ƒå¼åˆå§‹åŒ–ï¼ˆå­ç±»å¯é€‰é‡å†™ï¼‰"""
        # ğŸ”¥ é»˜è®¤ä¸ºç©ºå®ç°ï¼Œä¸å½±å“NPUç­‰ç°æœ‰é€»è¾‘
        pass
    
    def _init_distributed(self):
        """åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–"""
        if not dist.is_initialized():
            backend = self._get_backend()
            # ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨env://æ–¹å¼ï¼Œé…åˆtorchrun
            dist.init_process_group(backend=backend, init_method="env://")
            logger.info(f"Rank {self.rank}: Distributed initialized with {backend}")

    def sync(self):
        """åˆ†å¸ƒå¼åŒæ­¥å±éšœ"""
        if self.world_size > 1 and dist.is_initialized():
            try:
                dist.barrier(timeout=timedelta(seconds=30))  # ğŸ”¥ æ·»åŠ è¶…æ—¶
            except Exception as e:
                logger.warning(f"Rank {self.rank}: Sync barrier failed: {e}")
    
    def generate_video(self, request, task_id: str, progress_callback: Optional[Callable] = None) -> str:
        """ç”Ÿæˆè§†é¢‘ä¸»æµç¨‹"""
        logger.info(f"Rank {self.rank}: Start video generation for task {task_id}")
        try:
            # ä¸‹è½½å›¾ç‰‡ï¼ˆåªåœ¨rank=0ï¼‰
            image_path = None
            if self.rank == 0:
                image_url = getattr(request, 'image_path', getattr(request, 'image_url', None))
                image_path = self._download_image_sync(image_url, task_id)
            # å¹¿æ’­å›¾ç‰‡è·¯å¾„
            image_path = self._broadcast_image_path(image_path)
            # åŒæ­¥
            self.sync()
            # ç”Ÿæˆè§†é¢‘
            output_path = self._get_output_path(task_id)
            result_path = self._generate_video_common(request, image_path, output_path, progress_callback)
            logger.info(f"Rank {self.rank}: Video generation completed: {result_path}")
            return result_path
        except Exception as e:
            logger.error(f"Rank {self.rank}: Video generation failed: {str(e)}")
            self._empty_cache()
            raise

    def batch_generate_video(self, requests: List[Any], task_ids: List[str], progress_callback: Optional[Callable] = None) -> list:
        """æ‰¹é‡ç”Ÿæˆè§†é¢‘ä¸»æµç¨‹ï¼ˆå¯é€‰å®ç°ï¼‰"""
        results = []
        for request, task_id in zip(requests, task_ids):
            try:
                result = self.generate_video(request, task_id, progress_callback)
                results.append(result)
            except Exception as e:
                logger.error(f"Batch task {task_id} failed: {e}")
                results.append(None)
        return results

    def reload_model(self):
        """çƒ­æ›´æ–°æ¨¡å‹"""
        logger.info(f"Rank {self.rank}: Reloading model...")
        self.model = self._load_model()
        logger.info(f"Rank {self.rank}: Model reloaded.")

    def _broadcast_image_path(self, image_path: Optional[str]) -> str:
        """ç”¨åˆ†å¸ƒå¼å¹¿æ’­åŒæ­¥å›¾ç‰‡è·¯å¾„"""
        if self.world_size <= 1:
            return image_path

        import torch.distributed as dist
        if not dist.is_initialized():
            return image_path

        # ğŸ”¥ çœŸæ­£çš„åˆ†å¸ƒå¼å¹¿æ’­
        obj_list = [image_path]
        dist.broadcast_object_list(obj_list, src=0)
        logger.info(f"Rank {self.rank}: Image path broadcast completed")
        return obj_list[0]

    def _download_image_sync(self, image_url: str, task_id: str) -> str:
        """åŒæ­¥ä¸‹è½½å›¾ç‰‡"""
        output_dir = Path("generated_videos")
        output_dir.mkdir(exist_ok=True)
        image_path = output_dir / f"{task_id}_input.jpg"
        response = requests.get(image_url, timeout=60)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content)).convert("RGB")
        image.save(image_path, quality=95, optimize=True)
        logger.info(f"Image downloaded and saved: {image_path}")
        return str(image_path)

    def _get_output_path(self, task_id: str) -> str:
        output_dir = Path("generated_videos")
        output_dir.mkdir(exist_ok=True)
        return str(output_dir / f"{task_id}.mp4")

    def _generate_video_common(self, request, image_path: str, output_path: str, progress_callback: Optional[Callable] = None) -> str:
        """æ¨¡æ¿æ–¹æ³•ï¼šè°ƒç”¨è®¾å¤‡ç‰¹å®šç”Ÿæˆé€»è¾‘å¹¶ä¿å­˜è§†é¢‘"""
        self._log_memory_usage()
        
        # ğŸ”¥ æ‰€æœ‰rankéƒ½åŠ è½½å›¾ç‰‡
        img = Image.open(image_path).convert("RGB")
        
        # ğŸ”¥ å…³é”®ï¼šæ‰€æœ‰rankéƒ½å‚ä¸è®¡ç®—
        video_tensor = self._generate_video_device_specific(request, img, progress_callback)
        
        # ğŸ”¥ åªæœ‰rank 0ä¿å­˜è§†é¢‘
        if self.rank == 0 and video_tensor is not None:
            self._save_video(video_tensor, output_path)
            logger.info(f"Video saved to {output_path}")
        
        # ğŸ”¥ åˆ†å¸ƒå¼åŒæ­¥
        if self.world_size > 1:
            import torch.distributed as dist
            if dist.is_initialized():
                dist.barrier()
        
        return f"/videos/{os.path.basename(output_path)}"
        
    @abc.abstractmethod
    def _get_backend(self) -> str:
        """è¿”å›åˆ†å¸ƒå¼åç«¯ï¼Œå¦‚'nccl'æˆ–'gloo'"""
        pass

    @abc.abstractmethod
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        pass

    @abc.abstractmethod
    def _generate_video_device_specific(self, request, img, progress_callback: Optional[Callable] = None):
        """è®¾å¤‡ç‰¹å®šçš„è§†é¢‘ç”Ÿæˆé€»è¾‘"""
        pass

    @abc.abstractmethod
    def _save_video(self, video_tensor, output_path: str):
        """ä¿å­˜è§†é¢‘"""
        pass

    @abc.abstractmethod
    def _log_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨"""
        pass

    @abc.abstractmethod
    def _empty_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        pass